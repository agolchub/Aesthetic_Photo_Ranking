{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file is \" model\n",
      "Input image size is \" 1024 ,  680\n",
      "--------------------\n",
      "\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 298, 297, 32)      1115168   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 298, 297, 32)      128       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 298, 297, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 117, 116, 64)      8921152   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 117, 116, 64)      256       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 117, 116, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 51, 51, 128)       2097280   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 51, 51, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 51, 51, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 23, 23, 256)       1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 23, 23, 256)       1024      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 23, 23, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 21, 21, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 21, 21, 512)       2048      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 21, 21, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 225792)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               57803008  \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 72,759,649\n",
      "Trainable params: 72,757,665\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'model', errno = 21, error message = 'Is a directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-031705b2691e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mrun_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ohpc/pub/apps/python3/3.6.10-tf1.14.0/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \"\"\"\n\u001b[0;32m-> 1211\u001b[0;31m     \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ohpc/pub/apps/python3/3.6.10-tf1.14.0/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format)\u001b[0m\n\u001b[1;32m    111\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    112\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 113\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ohpc/pub/apps/python3/3.6.10-tf1.14.0/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ohpc/pub/apps/python3/3.6.10-tf1.14.0/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ohpc/pub/apps/python3/3.6.10-tf1.14.0/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'model', errno = 21, error message = 'Is a directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import math, sys, getopt\n",
    "\n",
    "modelout = 'model'\n",
    "imagewidth = 1024\n",
    "imageheight = 680\n",
    "\n",
    "print ('Output file is \"', modelout)\n",
    "print ('Input image size is \"', imagewidth, \", \", imageheight)\n",
    "\n",
    "if(modelout == '' or imagewidth == 0 or imageheight == 0):\n",
    "    print('Missing required parameter.')\n",
    "    print ('train.py -o <modelout> -w <inputwidth> -h <inputheight>')\n",
    "    sys.exit(2)\n",
    "\n",
    "print ('--------------------\\n\\n')\n",
    "\n",
    "K.image_data_format()\n",
    "\n",
    "img_width, img_height = imagewidth, imageheight\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (132, 88),input_shape=(img_width, img_height, 3), strides=(3,2), activation=\"relu\", kernel_initializer=\"he_uniform\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Conv2D(64, (66, 66), strides=(2,2), activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Conv2D(128, (16, 16), strides=(2,2), activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Conv2D(256, (7, 7), strides=(2,2), activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3, 3), strides=(1,1), activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(256, activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(128, activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, input_dim=1, kernel_initializer=\"he_uniform\", activation=\"linear\"))\n",
    "\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0))\n",
    "\n",
    "model.summary() \n",
    "\n",
    "run_filepath = modelout\n",
    "model.save(run_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-26f77531763b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!/usr/bin/env python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import skimage\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import sys, getopt\n",
    "from tensorflow.keras import initializers, models, optimizers\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform.tf_logging import error\n",
    "\n",
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,path,\n",
    "                 input_size=(1024, 680, 3),\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        self.path = path\n",
    "        self.items = glob(os.path.join(self.path,\"*.jpg\"))\n",
    "        self.n = len(self.items)\n",
    "        \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #import random\n",
    "\n",
    "        x = [] # images as arrays\n",
    "        y = [] # labels Infiltration or Not_infiltration\n",
    "        WIDTH = 1024\n",
    "        HEIGHT = 680\n",
    "        j = 0\n",
    "        images = []\n",
    "        rawscore = 0.0\n",
    "\n",
    "        for i in range(index*self.batch_size, (index+1)*self.batch_size):\n",
    "            item = self.items[index]\n",
    "            #print(\"Reading \" + item)\n",
    "            # Read and resize image\n",
    "            full_size_image = io.imread(item)\n",
    "            rawscore = int(os.path.basename(item).split(\"-\")[0])\n",
    "            out = rawscore\n",
    "            y.append(out)\n",
    "            images.append(item)\n",
    "            resizedImage = resize(full_size_image, (WIDTH,HEIGHT), anti_aliasing=True) \n",
    "            if(len(resizedImage.shape) < 3):\n",
    "                resizedImage = skimage.color.gray2rgb(resizedImage)\n",
    "\n",
    "            x.append(resizedImage)\n",
    "        return np.array(x),np.array(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "class WaitCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            import time\n",
    "\n",
    "            stream = os.popen(\"nvidia-smi -q -i 0 -d TEMPERATURE|grep 'GPU Current'|cut -d\\\" \\\" -f 30\")\n",
    "            temp = int(stream.read())\n",
    "            print(\"The temperature of the GPU is \",temp)\n",
    "\n",
    "            if(temp>75):\n",
    "                print(\"Cooling down...\")\n",
    "                while(temp>55):\n",
    "                    time.sleep(15)\n",
    "                    stream = os.popen(\"nvidia-smi -q -i 0 -d TEMPERATURE|grep 'GPU Current'|cut -d\\\" \\\" -f 30\")\n",
    "                    temp = int(stream.read())\n",
    "\n",
    "                print(\"Resuming -> \")\n",
    "\n",
    "            return super().on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "def proc_image_dir(Images_Path):\n",
    "    \n",
    "#    image_classes = sorted([dirname for dirname in os.listdir(Images_Path)\n",
    "#                      if os.path.isdir(os.path.join(Images_Path, dirname)) and not dirname.startswith(\".\") and not dirname.startswith(\"mblur\")])\n",
    "    \n",
    "#    print(image_classes)\n",
    "    \n",
    "    x = [] # images as arrays\n",
    "    y = [] # labels Infiltration or Not_infiltration\n",
    "    WIDTH = 1024\n",
    "    HEIGHT = 680\n",
    "  \n",
    "    print(\"Adding Images: \",end=\"\")\n",
    "    i = 0\n",
    "#    for image_class in image_classes:\n",
    "    #print(\"Processing \", image_class)\n",
    "    items = glob(os.path.join(Images_Path,\"*\"))\n",
    "    j = 0\n",
    "    images = []\n",
    "    rawscore = 0.0\n",
    "\n",
    "    for item in items:\n",
    "        print(\"Reading \"+item)\n",
    "        if item.lower().endswith(\".jpg\") or item.lower().endswith(\".bmp\"):\n",
    "            # Read and resize image\n",
    "            full_size_image = io.imread(item)\n",
    "\n",
    "            rawscore = int(os.path.basename(item).split(\"-\")[0])\n",
    "\n",
    "            if rawscore >= 1:\n",
    "                #x.append(full_size_image)\n",
    "                out = rawscore\n",
    "                print(out)\n",
    "                y.append(out)\n",
    "                images.append(item)\n",
    "                resizedImage = resize(full_size_image, (WIDTH,HEIGHT), anti_aliasing=True) \n",
    "                if(len(resizedImage.shape) < 3):\n",
    "                    resizedImage = skimage.color.gray2rgb(resizedImage)\n",
    "\n",
    "                x.append(resizedImage)\n",
    "                j+=1\n",
    "            #if j>3:\n",
    "            #    break\n",
    "                \n",
    "\n",
    "    print(\"\\nRead \" + str(j) + \" images.\\n\\n\")\n",
    "    return x,y,images\n",
    "\n",
    "def init_layer(layer):\n",
    "    try:\n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        import keras.backend as K\n",
    "        session = K.get_session()\n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            print(\"initializing kernel weights\")\n",
    "            layer.kernel.initializer.run(session=session)\n",
    "        if hasattr(layer, 'bias_initializer'):\n",
    "            print(\"initializing bias weights\")\n",
    "            layer.bias.initializer.run(session=session) \n",
    "        print(layer.name,\" re-initilized\")\n",
    "    except:\n",
    "        print(layer.name, \" could not be re-initilized\", sys.exc_info())\n",
    "\n",
    "def train(modelin,modelout,imagepath,epochs,batch_size,lr,decay,nesterov,checkpoint_filepath,train_path,val_path,transfer_learning,randomize_weights):\n",
    "    #load model\n",
    "    model = models.load_model(modelin)\n",
    "\n",
    "    if transfer_learning:\n",
    "        for layer in model.layers:\n",
    "            if (isinstance(layer, tf.keras.layers.Conv2D)):\n",
    "                print(\"conv layer\",layer.name)\n",
    "                layer.trainable = False\n",
    "\n",
    "    if randomize_weights:\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            if(layer.trainable):\n",
    "                init_layer(layer)\n",
    "\n",
    "    model.build()\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    if(imagepath!=''):\n",
    "        #load images\n",
    "        x2,y2,images = proc_image_dir(imagepath)\n",
    "        \n",
    "        # First split the data in two sets, 60% for training, 40% for Val/Test)\n",
    "        X_train, X_valtest, y_train, y_valtest = train_test_split(x2,y2, test_size=0.4, random_state=1)\n",
    "\n",
    "        # Second split the 40% into validation and test sets\n",
    "        X_test, X_val, y_test, y_val = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=1)\n",
    "    else:\n",
    "        training_generator = CustomDataGen(batch_size,train_path)\n",
    "        validation_generator = CustomDataGen(batch_size,val_path)\n",
    "        #X_train,y_train,image_list_train = proc_image_dir(train_path)\n",
    "        #X_val,y_val,image_list_val = proc_image_dir(val_path)\n",
    "\n",
    "    #run training loop\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,   \n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "    wait_callback = WaitCallback()\n",
    "\n",
    "    model.compile(\n",
    "        loss='mae',\n",
    "        optimizer=optimizers.SGD(learning_rate=lr,momentum = 0.0, decay=decay, nesterov=nesterov))\n",
    "    #history = model.fit(np.array(X_train), np.array(y_train),\n",
    "    #    validation_data=(np.array(X_val), np.array(y_val)),\n",
    "    #        epochs=epochs, batch_size=batch_size,\n",
    "    #        callbacks=[model_checkpoint_callback,wait_callback])\n",
    "\n",
    "    history = model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=epochs,callbacks=[model_checkpoint_callback,wait_callback],\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6)\n",
    "\n",
    "    #save model\n",
    "    model.save(modelout)\n",
    "\n",
    "    print ([model.history.history[\"loss\"],model.history.history[\"val_loss\"]])\n",
    "\n",
    "modelin = 'model'\n",
    "modelout = 'model.trained'\n",
    "imagepath = ''\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "lr = .001\n",
    "decay = 0.0001\n",
    "nesterov = False\n",
    "checkpoint_filepath = \"./checkpoint/\"\n",
    "train_path = './databaserelease2/NatureDataset/train'\n",
    "val_path = './databaserelease2/NatureDataset/val'\n",
    "transfer_learning = False\n",
    "randomize_weights = False\n",
    "\n",
    "print ('Input file is \"', modelin)\n",
    "print ('Output file is \"', modelout)\n",
    "print ('Image path is \"', imagepath)\n",
    "\n",
    "if(modelin == '' or modelout == '' or (imagepath == '' and (train_path == '' or val_path == ''))):\n",
    "    print('Missing required parameter.')\n",
    "    print ('train.py -i <modelin> -o <modelout> -p <imagepath>')\n",
    "    sys.exit(2)\n",
    "\n",
    "\n",
    "print ('--------------------\\n\\n')\n",
    "\n",
    "train(modelin,modelout,imagepath,epochs,batch_size,lr,decay,nesterov,checkpoint_filepath,train_path,val_path,transfer_learning,randomize_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.1",
   "language": "python",
   "name": "tensorflow-2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
